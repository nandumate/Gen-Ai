{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment No. 3**: Fine-tune GPT or GPT-2 for creative story generation.\n"
      ],
      "metadata": {
        "id": "KA7eUpFJNVhG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "cFQ8McOKeclx",
        "outputId": "75c0a3d3-b596-4902-a9fc-e0177dba645f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3/3 01:33, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>3.327637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>3.255084</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>3.238470</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time in a distant kingdom, the queen would be a little afraid that the kingdom might fall into the hands of the demons. But the queen could not be certain that she would be safe, and so she resolved to seek her own safety in her own right.\n",
            "\n",
            "Thereupon, when the king, seeing the queen's fear, saw how great her fear was, he began to speak the words which she had spoken, and in the midst of his words he cried out:\n",
            "\n",
            "\"My mother is still in the midst of the world, and she is still in the midst of the earth, but she is still in the midst of all the living. But I know that she is still in the midst of the world. She is still in the midst of all the living, but she is still in the midst of all the living. And so, my mother is still in the midst of all the living, but she is still in the midst of all the living.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import os\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Disable Weights & Biases (W&B) logging\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# Load GPT-2 Tokenizer and Model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Set pad token\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
        "\n",
        "# Load Dataset (Replace with your dataset or use a text file)\n",
        "dataset = load_dataset(\"tiny_shakespeare\")  # Example dataset\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"epoch\",  # Updated from evaluation_strategy\n",
        "    save_strategy=\"epoch\",\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    save_total_limit=2,\n",
        ")\n",
        "\n",
        "# Data collator to handle padding\n",
        "collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,  # Disable masked language modeling since GPT-2 is causal\n",
        ")\n",
        "\n",
        "# Trainer initialization\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=collator,  # Removed tokenizer argument\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()\n",
        "\n",
        "# Save fine-tuned model\n",
        "model.save_pretrained(\"./fine_tuned_gpt2\")\n",
        "tokenizer.save_pretrained(\"./fine_tuned_gpt2\")\n",
        "\n",
        "# Generate a story\n",
        "def generate_story(prompt, max_length=200):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "    outputs = model.generate(inputs, max_length=max_length, temperature=0.7, top_k=50, top_p=0.95, do_sample=True)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Example prompt\n",
        "prompt = \"Once upon a time in a distant kingdom,\"\n",
        "story = generate_story(prompt)\n",
        "print(story)\n"
      ]
    }
  ]
}